{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810916c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "\n",
    "def plot_cumulative_success(history_df, prompts, title=\"Cumulative Success Rate\", marker_size=3, evolution_start=None):\n",
    "    \"\"\"Plot cumulative success rate for each prompt with optional evolution marker.\"\"\"\n",
    "    plt.figure(figsize=(12 if evolution_start else 10, 6))\n",
    "    colors = {}\n",
    "    \n",
    "    prompt_set = set(history_df[\"chosen_prompt\"]) if evolution_start else prompts\n",
    "    for prompt in prompt_set:\n",
    "        prompt_hist = history_df[history_df[\"chosen_prompt\"] == prompt]\n",
    "        if len(prompt_hist) > 0:\n",
    "            cumsum = prompt_hist[\"reward\"].cumsum() / (np.arange(len(prompt_hist)) + 1)\n",
    "            x_vals = prompt_hist.index if evolution_start else cumsum.index\n",
    "            label = (prompt[:40] + \"...\") if len(prompt) > 40 else prompt\n",
    "            line = plt.plot(x_vals, cumsum, label=label, marker='o', markersize=marker_size, alpha=0.7)\n",
    "            colors[prompt] = line[0].get_color()\n",
    "    \n",
    "    if evolution_start:\n",
    "        plt.axvline(x=evolution_start, color='red', linestyle='--', linewidth=2, label='Evolution Start')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Trial Number\")\n",
    "    plt.ylabel(\"Observed Success Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return colors\n",
    "\n",
    "def plot_beta_distributions(results, prompts, colors, show_prior=True, original_winner=None, original_alpha=None, original_beta=None):\n",
    "    \"\"\"Plot Beta distributions (prior/posterior or evolution comparison).\"\"\"\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    if show_prior:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        # Prior\n",
    "        axes[0].set_title(\"Initial Beta Distributions (Prior)\")\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            axes[0].plot(x, np.ones_like(x), label=prompt, color=colors[prompt], alpha=0.7)\n",
    "        axes[0].set_xlabel(\"Success Rate\")\n",
    "        axes[0].set_ylabel(\"Density\")\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Posterior\n",
    "        axes[1].set_title(\"Final Beta Distributions (Posterior)\")\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            y = beta.pdf(x, results.loc[i, \"successes\"] + 1, results.loc[i, \"failures\"] + 1)\n",
    "            axes[1].plot(x, y, label=prompt, color=colors[prompt], alpha=0.7, linewidth=2)\n",
    "        axes[1].set_xlabel(\"Success Rate\")\n",
    "        axes[1].set_ylabel(\"Density\")\n",
    "        axes[1].legend()\n",
    "    else:\n",
    "        # Evolution comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        if original_winner and original_alpha and original_beta:\n",
    "            y_orig = beta.pdf(x, original_alpha, original_beta)\n",
    "            label = f\"{original_winner[:40]}... (Original)\" if len(original_winner) > 40 else f\"{original_winner} (Original)\"\n",
    "            plt.fill_between(x, y_orig, alpha=0.2, color='gray', label=label)\n",
    "            plt.plot(x, y_orig, color='gray', alpha=0.4, linewidth=2, linestyle='--')\n",
    "        \n",
    "        for i, prompt in enumerate(results[\"prompt\"]):\n",
    "            if prompt != original_winner:\n",
    "                y = beta.pdf(x, results.loc[i, \"successes\"] + 1, results.loc[i, \"failures\"] + 1)\n",
    "                label = (prompt[:40] + \"...\") if len(prompt) > 40 else prompt\n",
    "                plt.plot(x, y, label=label, alpha=0.7, linewidth=2)\n",
    "        \n",
    "        plt.title(\"Final Beta Distributions After Evolution\")\n",
    "        plt.xlabel(\"Success Rate\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7372f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------\n",
    "# Setup\n",
    "# ------------------------\n",
    "prompts = [\n",
    "    \"Hi, how are you today?\",\n",
    "    \"We have an amazing offer just for you!\",\n",
    "    \"Did you know we can save you time and money?\",\n",
    "    \"I'd love to show you something that could help your business.\",\n",
    "    \"What's your biggest challenge right now?\"\n",
    "]\n",
    "\n",
    "true_effectiveness = np.random.beta(2, 2, len(prompts))\n",
    "\n",
    "results = pd.DataFrame({\"prompt\": prompts, \"successes\": np.zeros(len(prompts)), \"failures\": np.zeros(len(prompts))})\n",
    "\n",
    "def get_reward(prompt_index, effectiveness_array):\n",
    "    \"\"\"Simulate customer response according to hidden effectiveness.\"\"\"\n",
    "    return 1 if random.random() < effectiveness_array[prompt_index] else 0\n",
    "\n",
    "# ------------------------\n",
    "# Thompson Sampling\n",
    "# ------------------------\n",
    "calls_per_day, days = 10, 30\n",
    "n_rounds = calls_per_day * days\n",
    "history = []\n",
    "\n",
    "for t in tqdm(range(n_rounds), desc=\"Simulating calls\"):\n",
    "    samples = [np.random.beta(results.loc[i, \"successes\"] + 1, results.loc[i, \"failures\"] + 1) for i in range(len(prompts))]\n",
    "    chosen = np.argmax(samples) # Choose which prompt to use\n",
    "    reward = get_reward(chosen, true_effectiveness)\n",
    "    \n",
    "    results.loc[chosen, \"successes\" if reward == 1 else \"failures\"] += 1\n",
    "    history.append({\"round\": t, \"chosen_prompt\": prompts[chosen], \"reward\": reward, \"true_effectiveness\": true_effectiveness[chosen]})\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "# ------------------------\n",
    "# Visualization\n",
    "# ------------------------\n",
    "colors = plot_cumulative_success(history_df, prompts, \"Cumulative Success Rate per Prompt (Simulated)\")\n",
    "plot_beta_distributions(results, prompts, colors, show_prior=True)\n",
    "\n",
    "# Print results\n",
    "results[\"estimated_rate\"] = results[\"successes\"] / (results[\"successes\"] + results[\"failures\"])\n",
    "print(\"True effectiveness:\", true_effectiveness)\n",
    "print(results)\n",
    "\n",
    "# Save winner for comparison\n",
    "initial_best_idx = results[\"estimated_rate\"].idxmax()\n",
    "original_winner = prompts[initial_best_idx]\n",
    "original_winner_alpha = results.loc[initial_best_idx, \"successes\"] + 1\n",
    "original_winner_beta = results.loc[initial_best_idx, \"failures\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import scipy.stats as sps\n",
    "import scipy.integrate as spi\n",
    "\n",
    "OPENAI_API_KEY = \"your-api-key-here\"\n",
    "OPENAI_MODEL_NAME = \"gpt-4.1-mini\" # If you cannot do it with 4.1-mini, you are doing something wrong ;)\n",
    "RETIREMENT_THRESHOLD = 0.05\n",
    "\n",
    "# ------------------------\n",
    "# Evolutionary Extension\n",
    "# ------------------------\n",
    "total_tokens_used = 0\n",
    "all_used_prompts = set(prompts)\n",
    "\n",
    "def mutate_prompt(base_prompt, existing_prompts):\n",
    "    \"\"\"Generate a variation using LLM.\"\"\"\n",
    "    global total_tokens_used\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates variations of sales prompts.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Create a single variation of this sales prompt: '{base_prompt}'. Avoid these: {existing_prompts}. Only respond with the new prompt.\"}\n",
    "            ],\n",
    "            max_tokens=50,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        total_tokens_used += response.usage.total_tokens\n",
    "        new_prompt = response.choices[0].message.content.strip()\n",
    "        if new_prompt not in existing_prompts:\n",
    "            return new_prompt\n",
    "    return f\"{new_prompt} (v{attempt})\"\n",
    "\n",
    "def prob_beta_mean_higher_numerical(alpha1, beta1, alpha2, beta2):\n",
    "    \"\"\"\n",
    "    Calculates the probability that a random variable drawn from the first\n",
    "    Beta distribution is greater than a random variable drawn from the second\n",
    "    Beta distribution. P(X1 > X2).\n",
    "    \"\"\"\n",
    "    def integrand(x):\n",
    "        return sps.beta.pdf(x, a=alpha1, b=beta1) * sps.beta.cdf(x, a=alpha2, b=beta2)\n",
    "    prob_X1_gt_X2, _ = spi.quad(integrand, 0, 1)\n",
    "    return prob_X1_gt_X2\n",
    "\n",
    "# Initialize evolution\n",
    "evolution_history = []\n",
    "active_prompts = prompts.copy()\n",
    "active_true_effectiveness = true_effectiveness.copy()\n",
    "\n",
    "# Create enhanced evolution results DataFrame with all necessary columns\n",
    "evolution_results = pd.DataFrame({\n",
    "    \"prompt\": active_prompts,\n",
    "    \"successes\": results[\"successes\"].values.copy(),\n",
    "    \"failures\": results[\"failures\"].values.copy(),\n",
    "    \"alpha\": results[\"successes\"].values.copy() + 1,\n",
    "    \"beta\": results[\"failures\"].values.copy() + 1,\n",
    "    \"estimated_rate\": 0.0,\n",
    "    \"ci_lower\": 0.0,\n",
    "    \"ci_upper\": 0.0\n",
    "})\n",
    "\n",
    "# Update derived columns\n",
    "evolution_results[\"estimated_rate\"] = evolution_results[\"alpha\"] / (evolution_results[\"alpha\"] + evolution_results[\"beta\"])\n",
    "\n",
    "for i in range(len(evolution_results)):\n",
    "    evolution_results.loc[i, \"ci_lower\"] = sps.beta.ppf(0.05, evolution_results.loc[i, \"alpha\"], evolution_results.loc[i, \"beta\"])\n",
    "    evolution_results.loc[i, \"ci_upper\"] = sps.beta.ppf(0.95, evolution_results.loc[i, \"alpha\"], evolution_results.loc[i, \"beta\"])\n",
    "\n",
    "for t in tqdm(range(n_rounds), desc=\"Evolving prompts\"):\n",
    "    # Thompson Sampling: sample from each prompt's beta distribution\n",
    "    samples = [np.random.beta(evolution_results.loc[i, \"alpha\"], evolution_results.loc[i, \"beta\"]) \n",
    "               for i in range(len(active_prompts))]\n",
    "    chosen = np.argmax(samples)\n",
    "    reward = get_reward(chosen, active_true_effectiveness)\n",
    "    \n",
    "    # Update successes/failures and derived columns\n",
    "    evolution_results.loc[chosen, \"successes\"] += reward\n",
    "    evolution_results.loc[chosen, \"failures\"] += (1 - reward)\n",
    "    evolution_results.loc[chosen, \"alpha\"] = evolution_results.loc[chosen, \"successes\"] + 1\n",
    "    evolution_results.loc[chosen, \"beta\"] = evolution_results.loc[chosen, \"failures\"] + 1\n",
    "    evolution_results.loc[chosen, \"estimated_rate\"] = evolution_results.loc[chosen, \"alpha\"] / (evolution_results.loc[chosen, \"alpha\"] + evolution_results.loc[chosen, \"beta\"])\n",
    "    evolution_results.loc[chosen, \"ci_lower\"] = sps.beta.ppf(0.05, evolution_results.loc[chosen, \"alpha\"], evolution_results.loc[chosen, \"beta\"])\n",
    "    evolution_results.loc[chosen, \"ci_upper\"] = sps.beta.ppf(0.95, evolution_results.loc[chosen, \"alpha\"], evolution_results.loc[chosen, \"beta\"])\n",
    "\n",
    "    evolution_history.append({\n",
    "        \"round\": t + n_rounds,\n",
    "        \"chosen_prompt\": active_prompts[chosen],\n",
    "        \"reward\": reward,\n",
    "        \"true_effectiveness\": active_true_effectiveness[chosen],\n",
    "        \"alpha\": evolution_results.loc[chosen, \"alpha\"],\n",
    "        \"beta\": evolution_results.loc[chosen, \"beta\"],\n",
    "        \"estimated_rate\": evolution_results.loc[chosen, \"estimated_rate\"]\n",
    "    })\n",
    "\n",
    "    # Every day (after 10 calls), retire weak prompts\n",
    "    if (t + 1) % 10 == 0:\n",
    "        # Find the best prompt (highest mean success rate)\n",
    "        best_idx = evolution_results[\"estimated_rate\"].idxmax()\n",
    "        best_alpha = evolution_results.loc[best_idx, \"alpha\"]\n",
    "        best_beta = evolution_results.loc[best_idx, \"beta\"]\n",
    "        best_mean = evolution_results.loc[best_idx, \"estimated_rate\"]\n",
    "        \n",
    "        retire_indices = []\n",
    "        \n",
    "        # Compare each prompt against the best prompt's beta distribution\n",
    "        for i in range(len(active_prompts)):\n",
    "            if i != best_idx:\n",
    "                current_alpha = evolution_results.loc[i, \"alpha\"]\n",
    "                current_beta = evolution_results.loc[i, \"beta\"]\n",
    "\n",
    "                # Calculate P(current_mean > best_mean) using the numerical integration function\n",
    "                # Note: Parameters are alpha1, beta1, alpha2, beta2.\n",
    "                # We want P(current > best), so current's params go first, best's params go second.\n",
    "                prob_current_greater_than_best = prob_beta_mean_higher_numerical(\n",
    "                    alpha1=current_alpha,\n",
    "                    beta1=current_beta,\n",
    "                    alpha2=best_alpha,\n",
    "                    beta2=best_beta\n",
    "                )\n",
    "\n",
    "                # Retire if there's less than a 5% chance that the current prompt is actually better than the best\n",
    "                # This means we're 95% confident the current prompt is NOT better than the best.\n",
    "                if prob_current_greater_than_best < RETIREMENT_THRESHOLD:\n",
    "                    retire_indices.append(i)\n",
    "\n",
    "        if len(retire_indices) > 0:\n",
    "            best_prompt = active_prompts[best_idx]\n",
    "            \n",
    "            for idx in retire_indices:\n",
    "                new_prompt = mutate_prompt(best_prompt, all_used_prompts)\n",
    "                active_prompts[idx] = new_prompt\n",
    "                all_used_prompts.add(new_prompt)\n",
    "                \n",
    "                # Perturb effectiveness\n",
    "                best_effectiveness = active_true_effectiveness[best_idx]\n",
    "                variation = np.random.uniform(-0.2, 0.2)\n",
    "                active_true_effectiveness[idx] = np.clip(best_effectiveness * (1 + variation), 0, 1)\n",
    "                \n",
    "                # Inherit prior with uncertainty from best prompt\n",
    "                best_total = evolution_results.loc[best_idx, \"successes\"] + evolution_results.loc[best_idx, \"failures\"]\n",
    "                best_rate = evolution_results.loc[best_idx, \"estimated_rate\"]\n",
    "                new_total = max(10, best_total * 0.2)\n",
    "                \n",
    "                new_successes = best_rate * new_total\n",
    "                new_failures = new_total - new_successes\n",
    "                \n",
    "                # Update all columns for the new prompt\n",
    "                evolution_results.loc[idx, \"prompt\"] = new_prompt\n",
    "                evolution_results.loc[idx, \"successes\"] = new_successes\n",
    "                evolution_results.loc[idx, \"failures\"] = new_failures\n",
    "                evolution_results.loc[idx, \"alpha\"] = new_successes + 1\n",
    "                evolution_results.loc[idx, \"beta\"] = new_failures + 1\n",
    "                evolution_results.loc[idx, \"estimated_rate\"] = evolution_results.loc[idx, \"alpha\"] / (evolution_results.loc[idx, \"alpha\"] + evolution_results.loc[idx, \"beta\"])\n",
    "                evolution_results.loc[idx, \"ci_lower\"] = sps.beta.ppf(0.05, evolution_results.loc[idx, \"alpha\"], evolution_results.loc[idx, \"beta\"])\n",
    "                evolution_results.loc[idx, \"ci_upper\"] = sps.beta.ppf(0.95, evolution_results.loc[idx, \"alpha\"], evolution_results.loc[idx, \"beta\"])\n",
    "# ------------------------\n",
    "# Visualization: Evolution\n",
    "# ------------------------\n",
    "evolution_df = pd.DataFrame(evolution_history)\n",
    "full_history = pd.concat([history_df, evolution_df], ignore_index=True)\n",
    "\n",
    "plot_cumulative_success(full_history, None, \"Cumulative Success Rate with Evolution\", marker_size=2, evolution_start=n_rounds)\n",
    "\n",
    "evolution_results[\"estimated_rate\"] = evolution_results[\"successes\"] / (evolution_results[\"successes\"] + evolution_results[\"failures\"])\n",
    "print(\"\\nFinal Active Prompts after Evolution:\")\n",
    "print(evolution_results)\n",
    "\n",
    "plot_beta_distributions(evolution_results, None, None, show_prior=False, original_winner=original_winner, original_alpha=original_winner_alpha, original_beta=original_winner_beta)\n",
    "\n",
    "print(f\"\\nTotal OpenAI tokens used: {total_tokens_used}\")\n",
    "print(f\"Total unique prompts generated: {len(all_used_prompts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
